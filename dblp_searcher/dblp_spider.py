from bs4 import BeautifulSoup, SoupStrainer
import requests
import re
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

# paperï¼š ç‚¹å‡»è·å–bibtex
def get_bibtex_from_url(dblp_url):
    """
    æ ¹æ® DBLP æ–‡çŒ®æ¡ç›®çš„ URL è·å– BibTeX ä¿¡æ¯
    ä¾‹å¦‚è¾“å…¥: https://dblp.org/rec/conf/ciarp/RozendoRNNL23
    å®é™…æŠ“å–: https://dblp.org/rec/conf/ciarp/RozendoRNNL23.html?view=bibtex
    """
    bibtex_url = f"{dblp_url}?view=bibtex"
    try:
        response = requests.get(bibtex_url)
        response.raise_for_status()
        # soup = BeautifulSoup(response.text, "html.parser")
        soup = BeautifulSoup(response.text, 'lxml')

        # æ‰¾åˆ° <div id="bibtex-section" class="section">
        bibtex_div = soup.find("div", id="bibtex-section")
        if bibtex_div:
            pre_tag = bibtex_div.find("pre")
            if pre_tag:
                return pre_tag.text.strip()
            else:
                return "æœªæ‰¾åˆ° <pre> æ ‡ç­¾"
        else:
            return "æœªæ‰¾åˆ° bibtex-section div"
    except requests.RequestException as e:
        print(f"è·å– BibTeX æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "è¯·æ±‚é”™è¯¯"

# paperï¼š ç‚¹å‡»è·å–æ‘˜è¦
def get_abstract_by_doi(doi):
    url = f"https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}?fields=title,abstract,authors,year"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        return data.get("abstract", "âš ï¸ æ‰¾ä¸åˆ°æ‘˜è¦")
    else:
        return "âŒ Semantic Scholar æŸ¥è¯¢å¤±è´¥"

# conference: ç‚¹å‡»è·å–nå¹´ä¼šè®®è¿æ¥

def get_dblp_conference_links(index_url):
    """
    ä» dblp ä¼šè®® index é¡µé¢ä¸­æå–æœ€è¿‘ n ä¸ªä¼šè®®å¹´ä»½çš„é“¾æ¥å’Œåç§°ã€‚

    å‚æ•°ï¼š
        index_url: str - ä¼šè®® index é¡µ URLï¼Œä¾‹å¦‚ https://dblp.org/db/conf/cvpr/index.html
        recent_n: int - åªè¿”å›æœ€è¿‘çš„ n ä¸ªä¼šè®®ï¼ˆæŒ‰å¹´ä»½å€’åºï¼‰

    è¿”å›ï¼š
        List[Tuple[ä¼šè®®åç§°, ä¼šè®®é“¾æ¥]]
    """
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    response = requests.get(index_url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")

    # soup = BeautifulSoup(response.text, 'html.parser')
    soup = BeautifulSoup(response.text, 'lxml')
    # ä» URL æå–ä¼šè®®ç®€ç§°ï¼ˆå¦‚ cvprï¼‰
    match = re.search(r"/conf/([^/]+)/index\.html", index_url)
    if not match:
        raise ValueError("æ— æ³•ä» URL ä¸­è§£æä¼šè®®ç®€ç§°")
    conf_abbr = match.group(1).upper()

    results = []

    # æ‰¾åˆ°æ‰€æœ‰ li.entry.editor.toc æ¡ç›®
    toc_items = soup.find_all('li', class_='entry editor toc')

    for item in toc_items:
        link_tag = item.find('a', href=True)
        if not link_tag:
            continue
        link = link_tag['href']

        # æå–å¹´ä»½
        item_id = item.get("id", "")
        year_match = re.search(r"(\d{4})", item_id or link)
        if not year_match:
            continue
        year = int(year_match.group(1))
        conf_name = f"{conf_abbr} {year}"
        results.append((year, conf_name, link))

    # æŒ‰å¹´ä»½å€’åºæ’åºï¼Œåªä¿ç•™æœ€è¿‘ n ä¸ª
    results = sorted(results, key=lambda x: x[0], reverse=True)

    # å»æ‰å¹´ä»½ï¼Œåªè¿”å› (name, link)
    return [(name, link) for _, name, link in results]

# conference: ç‚¹å‡»è·å–nå·æœŸåˆŠè¿æ¥
def get_dblp_search_conference_links(index_url):
    """
    ä» dblp ä¼šè®® index é¡µé¢ä¸­æå–æœ€è¿‘ n ä¸ªä¼šè®®å¹´ä»½çš„é“¾æ¥å’Œåç§°ã€‚

    å‚æ•°ï¼š
        index_url: str - ä¼šè®® index é¡µ URLï¼Œä¾‹å¦‚ https://dblp.org/db/conf/cvpr/index.html
        recent_n: int - åªè¿”å›æœ€è¿‘çš„ n ä¸ªä¼šè®®ï¼ˆæŒ‰å¹´ä»½å€’åºï¼‰

    è¿”å›ï¼š
        List[Tuple[ä¼šè®®åç§°, ä¼šè®®é“¾æ¥]]
    """
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    response = requests.get(index_url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")

    # soup = BeautifulSoup(response.text, 'html.parser')
    soup = BeautifulSoup(response.text, 'lxml')

    # ä» URL æå–ä¼šè®®ç®€ç§°ï¼ˆå¦‚ cvprï¼‰
    match = re.search(r"/conf/([^/]+)/", index_url)
    if not match:
        raise ValueError("æ— æ³•ä» URL ä¸­è§£æä¼šè®®ç®€ç§°")
    conf_abbr = match.group(1).upper()

    results = []

    # æ‰¾åˆ°æ‰€æœ‰ li.entry.editor.toc æ¡ç›®
    toc_items = soup.find_all('li', class_='entry editor toc')

    for item in toc_items:
        link_tag = item.find('a', href=True)
        if not link_tag:
            continue
        link = link_tag['href']

        # æå–å¹´ä»½
        item_id = item.get("id", "")
        year_match = re.search(r"(\d{4})", item_id or link)
        if not year_match:
            continue
        year = int(year_match.group(1))
        conf_name = f"{conf_abbr} {year}"
        results.append((year, conf_name, link))

    # æŒ‰å¹´ä»½å€’åºæ’åºï¼Œåªä¿ç•™æœ€è¿‘ n ä¸ª
    results = sorted(results, key=lambda x: x[0], reverse=True)

    # å»æ‰å¹´ä»½ï¼Œåªè¿”å› (name, link)
    return [(name, link) for _, name, link in results]

def get_journal_volumes(index_url):
    """
    çˆ¬å–dblpæœŸåˆŠä¸»é¡µä¸­çš„æœŸåˆŠå·å·å’Œå¯¹åº”é“¾æ¥ã€‚

    å‚æ•°ï¼š
        index_url: str - æœŸåˆŠä¸»é¡µURLï¼Œå¦‚ https://dblp.org/db/journals/pami/index.html
        recent_n: int or None - è¿”å›æœ€è¿‘çš„nä¸ªå·å·ï¼Œé»˜è®¤è¿”å›å…¨éƒ¨

    è¿”å›ï¼š
        List[(å·å·å­—ç¬¦ä¸², é“¾æ¥å­—ç¬¦ä¸²)]ï¼Œä¾‹å¦‚ [('Volume 47: 2025', 'https://dblp.org/db/journals/pami/pami47.html'), ...]
    """
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(index_url, headers=headers)
    if r.status_code != 200:
        raise Exception(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{r.status_code}")

    # soup = BeautifulSoup(r.text, 'html.parser')
    soup = BeautifulSoup(r.text, 'lxml')

    volumes = []
    # æŸ¥æ‰¾æ‰€æœ‰<li>æ ‡ç­¾ä¸‹çš„<a>æ ‡ç­¾
    for li in soup.find_all('li'):
        for a_tag in li.find_all('a', href=True):
            if a_tag:
                href = a_tag['href']
                text = a_tag.get_text(strip=True)
                # åˆ¤æ–­ href æ˜¯å¦åŒ…å«æœŸåˆŠå·é¡µé“¾æ¥ï¼ˆç®€å•è¿‡æ»¤ï¼‰
                # if 'db/journals' in href and 'html' in href and text.lower().startswith('volume'):
                if 'db/journals' in href and 'html' in href:
                    volumes.append((text, href))

    # æŒ‰å·å·æ’åºï¼ˆå‡è®¾æ ¼å¼å›ºå®šï¼Œæå–æ•°å­—æ’åºï¼‰
    def extract_volume_num(volume_str):
        import re
        m = re.findall(r'\d+', volume_str)
        return [int(num) for num in m] if m else []

    volumes = sorted(volumes, key=lambda x: extract_volume_num(x[0]), reverse=True)

    return volumes

# authors: ç‚¹å‡»è·å–ä½œè€…æ‰€æœ‰è®ºæ–‡

def crawl_dblp_profile(url):

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept-Encoding": "gzip, deflate"
    }

    response = requests.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'lxml',parse_only=SoupStrainer('ul', class_='publ-list'))
    papers = []
    current_year = None

    for li in soup.find_all('li', class_='entry'):
        # æ›´æ–°å¹´ä»½ä¿¡æ¯
        if 'class' in li.attrs and li['class'] == ['year']:
            current_year = li.text.strip()
            continue

        # åªå¤„ç†è®ºæ–‡æ¡ç›®
        if 'class' in li.attrs and 'entry' in li['class']:
            try:
                # æ ‡é¢˜
                title_tag = li.find('span', class_='title')
                title = title_tag.text.strip() if title_tag else "N/A"

                # ä½œè€…
                author_tags = li.find_all('span', itemprop='author')
                authors = [a.text.strip() for a in author_tags] if author_tags else []

                # å‘è¡¨ venue
                venue_tag = li.find('span', class_='venue')
                venue = venue_tag.text.strip() if venue_tag else "N/A"

                # é¡µç 
                pages_tag = li.find('span', itemprop='pagination')
                pages = pages_tag.text.strip() if pages_tag else "N/A"

                # ç±»å‹
                type_classes = li.get('class', [])
                type_ = next((cls for cls in type_classes if cls != 'entry'), 'N/A')

                # è®¿é—®æƒé™
                access = "open access" if li.find('img', alt='open access') else "N/A"

                # dblp key
                key = li.get('id', 'N/A')

                # DOI å’Œç”µå­ç‰ˆé“¾æ¥
                doi = "N/A"
                ee = "N/A"
                for a_tag in li.find_all('a', href=True):
                    href = a_tag['href']
                    if href.startswith('https://doi.org/'):\
                        doi = re.sub(r"https?://doi\.org/", "", href)
                    elif 'electronic edition' in a_tag.text.lower():
                        ee = href

                # è¯¦æƒ…é¡µ URL
                detail_tag = li.find('a', href=re.compile(r'/rec/'))
                url = f"{detail_tag['href']}" if detail_tag else "N/A"

                # å·å·
                volume_tag = li.find('span', itemprop='volumeNumber')
                volume = volume_tag.text.strip() if volume_tag else "N/A"

                entry = {
                    "title": title,
                    "authors": authors,
                    "venue": venue,
                    "pages": pages,
                    "year": current_year,
                    "type": type_,
                    "access": access,
                    "key": key,
                    "doi": doi,
                    "ee": ee,
                    "url": url,
                    "volume": volume
                }

                papers.append(entry)
            except Exception as e:
                print(f"Error parsing entry: {e}")

    return papers


def parse_dblp_entries(entries):
    result_list = []
    try:
        for item in entries:
            entry = {
                "title": item.get("title", "N/A"),
                "authors": item.get("authors", []),
                "venue": item.get("venue", "N/A"),
                "pages": item.get("pages", "N/A"),
                "year": item.get("year", "N/A"),
                "type": item.get("type", "N/A"),
                "access": item.get("access", "N/A"),
                "key": item.get("key", "N/A"),
                "doi": item.get("doi", "N/A"),
                "ee": item.get("ee", "N/A"),
                "url": item.get("url", "N/A"),
                "volume": item.get("volume", "N/A"),
            }
            result_list.append(entry)
    except Exception as e:
        print(f"è§£ææ¡ç›®æ—¶å‡ºé”™: {e}")
    return result_list


# if __name__ == "__main__":
#     # ç¤ºä¾‹ DOIï¼ˆä½ ä¹Ÿå¯ä»¥æ¢æˆä»»æ„ä¸€ä¸ªçœŸå® DOIï¼‰
#     example_doi = "10.1109/CVPR52688.2022.01846"  # æ¥è‡ªCVPR 2022çš„ä¸€ç¯‡è®ºæ–‡
#     print(f"ğŸ” æ­£åœ¨è·å–æ‘˜è¦ï¼ŒDOI: {example_doi}")
#
#     abstract = get_abstract_by_doi(example_doi)
#
#     print("\nğŸ“„ è·å–åˆ°çš„æ‘˜è¦ï¼š")
#     print(abstract)
#
#     print(get_bibtex_from_url('https://dblp.org/rec/conf/ciarp/RozendoRNNL23'))
#
#
#     url = "https://dblp.org/db/conf/iclr/index.html"
#     latest_confs = get_dblp_conference_links(url, recent_n=10)
#     for name, link in latest_confs:
#         print(f"{name}: {link}")

        # url = "https://dblp.org/db/journals/pami/index.html"
        #     vols = get_journal_volumes(url, recent_n=5)
        #     for name, link in vols:
        #         print(f"{name}: {link}")
        #
        # url = "https://dblp.org/pid/202/1700.html"
        # results = crawl_dblp_profile(url)
        #
        # for idx, paper in enumerate(results, 1):
        #     print(f"{idx}. {paper['title']} ({paper['year']})")
        #     print(f"   Authors: {', '.join(paper['authors'])}")
        #     print(f"   Venue: {paper['venue']}")
        #     print(f"   Pages: {paper['pages']}")
        #     print(f"   Type: {paper['type']}")
        #     print(f"   Access: {paper['access']}")
        #     print(f"   Key: {paper['key']}")
        #     print(f"   DOI: {paper['doi']}")
        #     print(f"   EE: {paper['ee']}")
        #     print(f"   URL: {paper['url']}")
        #     print(f"   Volume: {paper['volume']}\n")
